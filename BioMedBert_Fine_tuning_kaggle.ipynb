{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioMedBert Fine tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1C6AYu1PGP7"
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqmkscQsPWy-",
        "outputId": "48762715-2e0d-4f8e-dec1-79694e9d7852"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKlfE4DkU_iG",
        "outputId": "2ac3814b-e1b2-4e5a-e9da-88742790da53"
      },
      "source": [
        "sentence = \"no acute cardiopulmonary findings.\"\n",
        "tokenise_text = tokenizer.tokenize(sentence)\n",
        "tokenise_text"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['no',\n",
              " 'acute',\n",
              " 'card',\n",
              " '##io',\n",
              " '##pu',\n",
              " '##lm',\n",
              " '##ona',\n",
              " '##ry',\n",
              " 'findings',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vseS06NVSnT",
        "outputId": "8d970656-4537-433c-ef6c-16988a2b3642"
      },
      "source": [
        "numerical_tokens = tokenizer.convert_tokens_to_ids(tokenise_text)\n",
        "numerical_tokens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 9505, 119]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "awLOJ5CBWyA0",
        "outputId": "29bbb192-b103-4354-821b-30a56e6e51d7"
      },
      "source": [
        "tokenizer.decode(numerical_tokens)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'no acute cardiopulmonary findings.'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhv4IRi4XDde",
        "outputId": "d76da128-7d98-48b3-f964-2339f9226370"
      },
      "source": [
        "#encoding\n",
        "tokenizer.encode(sentence)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 9505, 119, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaKrVNaXXSVl"
      },
      "source": [
        "# tokenizer.get_vocab()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QeIhJQpXboR"
      },
      "source": [
        "def create_semmentation_embedding(token_embeddings, sep_token_id):\n",
        "  segmentation_embeddings = []\n",
        "  sentence_length = 0\n",
        "\n",
        "  for token in token_embeddings:\n",
        "    segmentation_embeddings.append(sentence_length%2)\n",
        "    if token == sep_token_id:\n",
        "      sentence_length+=1\n",
        "  return segmentation_embeddings\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDLmpGTuZjig"
      },
      "source": [
        "sentence2 = \"tortuous aorta otherwise unremarkable examination.\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7VIUd91Zwb1",
        "outputId": "130446e8-b338-48e4-948a-406277ecdc38"
      },
      "source": [
        "encoded_text = tokenizer.encode(sentence, sentence2)\n",
        "print(encoded_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1185, 12104, 3621, 2660, 16091, 13505, 7637, 1616, 9505, 119, 102, 1106, 3740, 8163, 170, 12148, 1161, 4303, 8362, 16996, 23822, 1895, 8179, 119, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdsHB6-9Z8dD",
        "outputId": "ae194d5f-af35-406f-baea-8172ea8d51b5"
      },
      "source": [
        "#create segmentation embedding\n",
        "create_semmentation_embedding(encoded_text, 102)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egsIydmWaPam"
      },
      "source": [
        "sentence3 = \"no pulmonary disease\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz9Qzq6_awuA"
      },
      "source": [
        "# tokenizer.encode([sentence, sentence2, sentence3])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0RayTkCa3OE"
      },
      "source": [
        "# model?\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0t31OcRvDD8"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP7SleGTv0cj"
      },
      "source": [
        "Steps to perform:\n",
        "1. Input document\n",
        "2. Token Embeddings\n",
        "3. Segment Embeddings\n",
        "4. Position embeddings\n",
        "\n",
        "pass that to transoforrmer encoder layer -> generates contextual embeddings->pass that to generative decoder layer for sentence summarization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyigEsdJvwDy"
      },
      "source": [
        "def encode_text(text, tokenizer, symbols, is_summary=False):\n",
        "    if is_summary:\n",
        "        #don't need [CLS] and [SEP] token while inferencing.\n",
        "        encoded = [tokenizer.encode(s)[1:-1] for s in text]\n",
        "    else:\n",
        "        encoded = [tokenizer.encode(s) for s in text]\n",
        "        \n",
        "    flattened = [item for sublist in encoded for item in sublist]\n",
        "    \n",
        "    if is_summary:\n",
        "        return [symbols['BOS']] + flattened + [symbols['EOS']] \n",
        "    \n",
        "    return flattened"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TTI4xaPyW_C"
      },
      "source": [
        "def create_segmentation_embedding(token_embeddings, sep_token_id):\n",
        "  segmentation_embeddings = []\n",
        "  sentence_length = 0\n",
        "\n",
        "  for token in token_embeddings:\n",
        "    segmentation_embeddings.append(sentence_length%2)\n",
        "    if token == sep_token_id:\n",
        "      sentence_length+=1\n",
        "  return segmentation_embeddings"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41ryaX1MyY5F"
      },
      "source": [
        "def pad(encoded_text, seq_length, tokenizer, symbols, is_summary=False):\n",
        "    if len(encoded_text) > seq_length:\n",
        "        if is_summary:\n",
        "            encoded_text = encoded_text[:seq_length]\n",
        "        else:\n",
        "            sent_sep_idxs = [idx for idx, t in enumerate(encoded_text) if t == tokenizer.sep_token_id and idx < seq_length]\n",
        "            last_sent_sep_idx = min(max(sent_sep_idxs)+1 if (len(sent_sep_idxs) > 0) else seq_length, seq_length)\n",
        "            encoded_text = encoded_text[:last_sent_sep_idx]\n",
        "    \n",
        "    if len(encoded_text) < seq_length:\n",
        "        encoded_text.extend([tokenizer.pad_token_id] * (seq_length - len(encoded_text)))\n",
        "    \n",
        "    \n",
        "    if is_summary:\n",
        "        encoded_text += [tokenizer.pad_token_id]\n",
        "\n",
        "    return encoded_text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFrK7hVcyfsa"
      },
      "source": [
        "def create_mask(text_tensor):\n",
        "    \"\"\"create attention mask\"\"\"\n",
        "    mask = torch.zeros_like(text_tensor)\n",
        "    mask[text_tensor != tokenizer.pad_token_id] = 1 \n",
        "    \n",
        "    return mask"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmaEDPIiylZ4"
      },
      "source": [
        "def collate_function(data, tokenizer, symbols, block_size, training):\n",
        "    encoded_stories = [encode_text(story, tokenizer, symbols) for _, story, summary in data]\n",
        "    encoded_summaries = [encode_text(summary, tokenizer, symbols, True) for _, story, summary in data]\n",
        "    story_segembs = [create_segmentation_embedding(s, tokenizer) for s in encoded_stories]\n",
        "        \n",
        "    padded_stories = torch.tensor([pad(s, block_size, tokenizer, symbols) for s in encoded_stories]).long()\n",
        "    padded_summaries = torch.tensor([pad(s, block_size, tokenizer, symbols, True) for s in encoded_summaries]).long()\n",
        "    padded_segembs = torch.tensor([pad(s, block_size, tokenizer, symbols) for s in story_segembs]).long()\n",
        "    \n",
        "    stories_mask = create_mask(padded_stories)\n",
        "    summaries_mask = create_mask(padded_summaries)\n",
        "    \n",
        "    if training:\n",
        "        return [padded_stories, padded_summaries, padded_segembs, stories_mask, summaries_mask], padded_summaries[:,1:]\n",
        "    else:\n",
        "        Batch = namedtuple(\"Batch\", [\"document_names\", \"batch_size\", \"src\", \"segs\", \"mask_src\", \"tgt_str\"])\n",
        "        names = [name for name, _, _ in data]\n",
        "        summaries = [\" \".join(summary_list) for _, _, summary_list in data]\n",
        "        batch = Batch(\n",
        "            document_names=names,\n",
        "            batch_size=len(encoded_stories),\n",
        "            src=padded_stories.to(args.device),\n",
        "            segs=padded_segembs.to(args.device),\n",
        "            mask_src=stories_mask.to(args.device),\n",
        "            tgt_str=summaries,\n",
        "        )\n",
        "        \n",
        "        return batch"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv7C4eIOzH0a",
        "outputId": "7c4ee945-6828-4e6c-f67d-bdef9867ce42"
      },
      "source": [
        "# clone the repo\n",
        "!git clone https://github.com/chandan5362/Abstractive-Text-Summarisation.git"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Abstractive-Text-Summarisation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioqEHb93y1JG"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision as tv\n",
        "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "2YlKXRwjzgis",
        "outputId": "fc8f01a6-762c-4925-8053-12bed41c56f2"
      },
      "source": [
        "# laod the dataset\n",
        "df = pd.read_csv(\"/content/Abstractive-Text-Summarisation/data/indiana/preprocessed-indiana-cxr-reports.csv\")\n",
        "df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COMPARISON</th>\n",
              "      <th>INDICATION</th>\n",
              "      <th>FINDINGS</th>\n",
              "      <th>IMPRESSION</th>\n",
              "      <th>indication_count</th>\n",
              "      <th>findings_count</th>\n",
              "      <th>impression_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>none.</td>\n",
              "      <td>no indication</td>\n",
              "      <td>heart size normal. lungs are clear. are normal...</td>\n",
              "      <td>normal chest</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>no comparison</td>\n",
              "      <td>slipped back on right side</td>\n",
              "      <td>the heart size and pulmonary vascularity appea...</td>\n",
              "      <td>no evidence of active disease.</td>\n",
              "      <td>5</td>\n",
              "      <td>36</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>no comparison</td>\n",
              "      <td>bone marrow transplant evaluation. aml.</td>\n",
              "      <td>the heart size and pulmonary vascularity appea...</td>\n",
              "      <td>no evidence of active disease.</td>\n",
              "      <td>5</td>\n",
              "      <td>38</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>none.</td>\n",
              "      <td>chest pain and .</td>\n",
              "      <td>the heart is normal in size and contour. the l...</td>\n",
              "      <td>no acute cardiopulmonary disease.</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>none.</td>\n",
              "      <td>mid to lower back pain since .</td>\n",
              "      <td>the heart is normal in size and contour. the l...</td>\n",
              "      <td>no acute cardiopulmonary disease.</td>\n",
              "      <td>7</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      COMPARISON  ... impression_count\n",
              "0          none.  ...                2\n",
              "1  no comparison  ...                5\n",
              "2  no comparison  ...                5\n",
              "3          none.  ...                4\n",
              "4          none.  ...                4\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH0bEcU1msmr"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nPZjeYdo1uZ"
      },
      "source": [
        "#setrting up hyperparameters\n",
        "hparams = {\n",
        "    \"batch_size\": 5,\n",
        "    \"encoder_max_seq_length\" : 512,\n",
        "    \"decoder_max_seq_length\" : 512,\n",
        "\n",
        "}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJlyZ0zBy9uP"
      },
      "source": [
        "class SummarisationDataset(Dataset):\n",
        "    def __init__(self, path, subset=None):\n",
        "        if path.endswith('.csv'):\n",
        "            self.dataset = pd.read_csv(path)\n",
        "        \n",
        "        if subset:\n",
        "            self.dataset = self.dataset.iloc[:subset]\n",
        "        \n",
        "    def __len__(self):\n",
        "         return self.dataset.shape[0]\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        title = \"\"\n",
        "        article = self.dataset.iloc[idx]['FINDINGS']\n",
        "        article = [s.strip()+'.' for s in article.split('.')]\n",
        "        article = [self.add_missing_period(line) for line in article if len(line) > 0]\n",
        "        article = [s for s in article if s not in ['..', '.']]\n",
        "        \n",
        "        summary = self.dataset.iloc[idx]['IMPRESSION']\n",
        "        summary = [s.strip()+'.' for s in summary.split('.')]\n",
        "        summary = [self.add_missing_period(line) for line in summary if len(line) > 0]\n",
        "        summary = [s for s in summary if s not in ['..', '.']]\n",
        "        unflat_story = [tokenizer.encode(s) for s in article]\n",
        "        unflat_summary = [tokenizer.encode(s) for s in summary]\n",
        "        flat_article = [item for sublist in unflat_story for item in sublist]\n",
        "        flat_summary = [item for sublist in unflat_summary for item in sublist]\n",
        "        padded_article = flat_article + [tokenizer.pad_token_id] * (hparams[\"encoder_max_seq_length\"] - len(flat_article))\n",
        "        padded_summary = flat_summary + [tokenizer.pad_token_id] * (hparams[\"decoder_max_seq_length\"] - len(flat_summary))\n",
        "\n",
        "        return np.asarray(padded_article), np.asarray(padded_summary)\n",
        "    \n",
        "    def add_missing_period(self, line):\n",
        "        END_TOKENS = [\".\", \"!\", \"?\", \"...\", \"'\", \"`\", '\"', u\"\\u2019\", u\"\\u2019\", \")\"]\n",
        "        if line.startswith(\"@highlight\"):\n",
        "            return line\n",
        "        if line[-1] in END_TOKENS and len(line):\n",
        "            return line\n",
        "        return line + \".\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U0qqRAc0Zkx"
      },
      "source": [
        "data = SummarisationDataset(\"/content/Abstractive-Text-Summarisation/data/indiana/preprocessed-indiana-cxr-reports.csv\", subset=1000)\n",
        "\n",
        "train_ds, test_ds = train_test_split(data, test_size=0.2)\n",
        "valid_ds, test_ds = train_test_split(test_ds, test_size=0.3)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txpaWykwlXr4"
      },
      "source": [
        "# art, sum = next(iter(train_ds))\n",
        "# sum.shape"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcQfGhSJ0ugW"
      },
      "source": [
        "# DataLoader\n",
        "train_loader = DataLoader(train_ds, batch_size=hparams[\"batch_size\"], shuffle = True)\n",
        "val_loader = DataLoader(valid_ds, batch_size=4)\n",
        "test_loader = DataLoader(test_ds, batch_size=4)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w16C_LVU013i"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F5eXedNB2Wi"
      },
      "source": [
        "## Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAUR3J0RDWIm"
      },
      "source": [
        "from transformers import EncoderDecoderModel"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKPv5ub8DFrZ",
        "outputId": "1118995c-2a1c-4e1e-d22b-aab1e144e391"
      },
      "source": [
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", \"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4CA9CnxIsLT"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHJvpNczd8zL"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHv3jLN8Tomd",
        "outputId": "7bde3385-51c8-45d4-a88f-bd66ce772d35"
      },
      "source": [
        "#import rouge score\n",
        "!pip install rouge_score"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Ut389eZiPs"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                      'The quick brown dog jumps on the log.')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi5e9Q-Oaa3Z",
        "outputId": "5a0e275d-ece1-442b-b0ca-7aa90b04f671"
      },
      "source": [
        "scores"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
              " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC53lpfkpwc8"
      },
      "source": [
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPqRoDAlp7RV"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpYgdYmroO0"
      },
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val   = 0\n",
        "        self.avg   = 0\n",
        "        self.sum   = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val   = val\n",
        "        self.sum   += val * n\n",
        "        self.count += n\n",
        "        self.avg   = self.sum / self.count"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZmLBfzBT2Uy"
      },
      "source": [
        "# # set special tokens\n",
        "# model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
        "# roberta_shared.config.eos_token_id = tokenizer.eos_token_id\n",
        "# roberta_shared.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# sensible parameters for beam search\n",
        "# set decoding params                               \n",
        "model.config.max_length = 30\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 1\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.repetition_penalty = 3.0\n",
        "model.config.num_beams = 10\n",
        "# model.config.vocab_size = roberta_shared.config.encoder.vocab_size"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZGxHm5p9Pnx"
      },
      "source": [
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVg0_Uvt9kSU"
      },
      "source": [
        "optimizer = torch.optim.Adam(params = model.parameters(), lr =2e-3)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8DAneYDagzc"
      },
      "source": [
        "for epoch in range(10):\n",
        "  batch_time = AverageMeter()\n",
        "\n",
        "  losses  = AverageMeter()\n",
        "  end = time.time()\n",
        "  for i, (article, summary) in enumerate(train_loader):\n",
        "\n",
        "    op = model(input_ids = article, labels = summary)\n",
        "    loss, out = op.loss, op.logits\n",
        "    losses.update(loss.item(), out.size(0))\n",
        "\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    op.loss.backward()\n",
        "\n",
        "    if i % 5 == 0:\n",
        "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "            .format(epoch+1, i, len(train_loader),  batch_time=batch_time,\n",
        "             loss=losses))\n",
        "      model.save_pretrained(\"bert2bert\")\n",
        "    optimizer.step()\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDDB8NVFSjd6"
      },
      "source": [
        "model = EncoderDecoderModel.from_pretrained(\"bert2bert\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn17DQbpeaTz"
      },
      "source": [
        "sentence = df[\"FINDINGS\"].values[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAbJj9f77BT6"
      },
      "source": [
        "findings, sum = next(iter(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUIRzVds7Vx3"
      },
      "source": [
        "input_ids = tokenizer(\"heart size normal. lungs are clear. are normal. no pneumonia effusions edema pneumothorax adenopathy nodules or masses.\", return_tensors=\"pt\").input_ids"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWaL-1GX70H6"
      },
      "source": [
        "input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qz4fJaD7XqK"
      },
      "source": [
        "model.generate(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5TthQ-i7t7t"
      },
      "source": [
        "input_ids = tokenizer(\"heart size normal. lungs are clear. are normal. no pneumonia effusions edema pneumothorax adenopathy nodules or masses\", return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(\"no pneumonia effusion\", return_tensors=\"pt\").input_ids\n",
        "outputs = model(input_ids=input_ids, labels=labels)\n",
        "loss, logits = outputs.loss, outputs.logits"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX9f3Cwf8pam",
        "outputId": "7e66cca9-8042-4333-8cc3-37b6f9d3267e"
      },
      "source": [
        "loss"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.0887, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiLM19GU8qqp",
        "outputId": "ca8f197f-877d-44a5-de45-70e328b046d1"
      },
      "source": [
        "generated = model.generate(labels)\n",
        "generated"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[101,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N6d_oaM8s_5",
        "outputId": "2193deae-95aa-42e2-ee50-1040c5060639"
      },
      "source": [
        "generated.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z70UnP3Uwc9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}